{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66887246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"/Users/haderie/Downloads/housing/train.csv\")\n",
    "train_df = train_df.drop(columns=[ \"zipcode\"])\n",
    "\n",
    "X_train = train_df.drop(columns=[\"price\"]) # features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "y_train = train_df[\"price\"] / 1000 # target\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(\"/Users/haderie/Downloads/housing/test.csv\")\n",
    "test_df = test_df.drop(columns=[\"id\", \"date\", \"zipcode\"])\n",
    "\n",
    "X_test = test_df.drop(columns=[\"price\"]) # featurs\n",
    "\n",
    "y_test = test_df[\"price\"] / 1000  # target\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb7a6c",
   "metadata": {},
   "source": [
    "In this problem, you will derive the optimal parameters for ridge regression and train ridge regression models with different regularization levels. In ridge regression, the loss function includes a regularization term:\n",
    "\n",
    "$J(\\theta) = \\sum_{i=1}^N(h_{\\theta}(x_i)-y_i)^2 + \\lambda \\sum_{j=1}^d \\theta_j^2$\n",
    "\n",
    "1. **[A]** Write the derivation of the closed form solution for parameter $\\theta$ that minimizes the loss function $J(\\theta)$ in ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf73a6f3-5d06-4699-912d-aad6c13e4c5f",
   "metadata": {},
   "source": [
    "In PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072636b",
   "metadata": {},
   "source": [
    "2. **[C]** Modify your implementation from Problem 5 to implement ridge regression with gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce894e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_gradient_descent(X, y, alpha, lam, num_iters):\n",
    "    \"\"\"\n",
    "    Gradient descent for ridge regression\n",
    "    X: (N, d) design matrix with intercept\n",
    "    y: (N,) target\n",
    "    lam: regularization parameter λ\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    theta = np.zeros(d)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        gradient = (2 / N) * X.T @ (X @ theta - y) + 2 * lam * theta\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2d403",
   "metadata": {},
   "source": [
    "3. **[C]** Simulate $N=1000$ values of random variable $X_i$, distributed uniformly on interval $[-2,2]$. Simulate the values of random variable       $Y_i = 1 + 2X_i + e_i$, where $e_i$ is drawn from a Gaussian distribution $N(0, 2)$. \n",
    "\n",
    "Fit this data with linear regression, and also with ridge regression for different values of $\\lambda \\in \\{1,10,100,1000,10000\\}$. \n",
    "\n",
    "Print the slope, the MSE values, and the $R^2$ statistic for each case and write down some observations. What happens as the regularization parameter $\\lambda$ increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8260681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "Intercept: 1.0405149813978518\n",
      "Slope: 1.9656920054163693\n",
      "MSE: 1.8653740489434376\n",
      "R^2: 0.7367593726211263\n",
      "\n",
      "Ridge (lambda=1)\n",
      "Slope: 1.017926323013737\n",
      "MSE: 3.419195822005353\n",
      "R^2: 0.5174848423426281\n",
      "\n",
      "Ridge (lambda=10)\n",
      "Slope: 0.23265695792026644\n",
      "MSE: 6.7702796662790465\n",
      "R^2: 0.04458161198758659\n",
      "\n",
      "Ridge (lambda=100)\n",
      "Slope: 0.026044838209081718\n",
      "MSE: 7.9465287822285235\n",
      "R^2: -0.12141005891175194\n",
      "\n",
      "Ridge (lambda=1000)\n",
      "Slope: 0.0026359727231450724\n",
      "MSE: 8.087221790538303\n",
      "R^2: -0.1412645839579565\n",
      "\n",
      "Ridge (lambda=10000)\n",
      "Slope: -0.0007501558600695277\n",
      "MSE: 8.107440944031175\n",
      "R^2: -0.1441179005104991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# simulate data\n",
    "N = 1000\n",
    "X = np.random.uniform(-2, 2, N)\n",
    "e = np.random.normal(0, np.sqrt(2), N)\n",
    "y = 1 + 2*X + e\n",
    "\n",
    "# matrix with intercept\n",
    "X_matrix = np.column_stack((np.ones(N), X))\n",
    "\n",
    "# linear regression\n",
    "theta = np.linalg.pinv(X_matrix) @ y\n",
    "y_pred_lr = X_matrix @ theta\n",
    "\n",
    "print(\"Linear Regression\")\n",
    "print(\"Intercept:\", theta[0])\n",
    "print(\"Slope:\", theta[1])\n",
    "print(\"MSE:\", mean_squared_error(y, y_pred_lr))\n",
    "print(\"R^2:\", r2_score(y, y_pred_lr))\n",
    "print()\n",
    "\n",
    "# ridge Regression \n",
    "lambdas = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "for lam in lambdas:\n",
    "    theta_ridge = ridge_gradient_descent(\n",
    "        X_matrix, y,\n",
    "        alpha=0.0001,\n",
    "        lam=lam,\n",
    "        num_iters=5000\n",
    "    )\n",
    "\n",
    "    y_pred_ridge = X_matrix @ theta_ridge\n",
    "\n",
    "    print(f\"Ridge (lambda={lam})\")\n",
    "    print(\"Slope:\", theta_ridge[1])\n",
    "    print(\"MSE:\", mean_squared_error(y, y_pred_ridge))\n",
    "    print(\"R^2:\", r2_score(y, y_pred_ridge))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c55c54",
   "metadata": {},
   "source": [
    "As the regularization parameter λ increases, the slope of the ridge regression model shrinks toward zero, the MSE increases, and the R² decreases. Small λ has little effect, while very large λ causes the model to underfit completely, effectively ignoring the input feature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
