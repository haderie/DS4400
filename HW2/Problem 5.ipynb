{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcaa75ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"/Users/haderie/Downloads/housing/train.csv\")\n",
    "train_df = train_df.drop(columns=[ \"zipcode\"])\n",
    "\n",
    "X_train = train_df.drop(columns=[\"price\"]) # features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "y_train = train_df[\"price\"] / 1000 # target\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(\"/Users/haderie/Downloads/housing/test.csv\")\n",
    "test_df = test_df.drop(columns=[\"id\", \"date\", \"zipcode\"])\n",
    "\n",
    "X_test = test_df.drop(columns=[\"price\"]) # featurs\n",
    "\n",
    "y_test = test_df[\"price\"] / 1000  # target\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b60bba",
   "metadata": {},
   "source": [
    "In this problem, you will implement your own gradient descent algorithm and apply it to linear regression on the same house prediction dataset.\n",
    "\n",
    "1. Write code for gradient descent for training linear regression using the algorithm from class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4c702ff-71e1-4695-b22d-9c257706fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(X, y, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Gradient descent for linear regression\n",
    "    X: (N, d) matrix with intercept\n",
    "    y: (N,) target values\n",
    "    alpha: learning rate\n",
    "    num_iters: number of iterations\n",
    "    \"\"\"\n",
    "    N, d = X.shape # num of data points, num of features\n",
    "    theta = np.zeros(d)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        gradient = (2 / N) * X.T @ (X @ theta - y)\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e825f",
   "metadata": {},
   "source": [
    "2. Vary the value of the learning rate (at least 3 different values $\\alpha \\in \\{0.01,0.1,0.5\\}$) and report the value of the model parameter $\\theta$ after different number of iterations (10, 50, and 100). Include in a table the MSE and $R^2$ metrics on the training and testing set for the different number of iterations and different learning rates. You can choose more values of the learning rates to observe how the\n",
    "behavior of the algorithm changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f687cbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   alpha  iterations      Train MSE      Train R^2       Test MSE  \\\n",
      "0   0.01          10   2.357311e+05  -1.047393e+00   2.828668e+05   \n",
      "1   0.01          50   6.969578e+04   3.946717e-01   9.432003e+04   \n",
      "2   0.01         100   3.676495e+04   6.806857e-01   6.127904e+04   \n",
      "3   0.10          10   3.504793e+04   6.955985e-01   6.000379e+04   \n",
      "4   0.10          50   3.142706e+04   7.270468e-01   5.889054e+04   \n",
      "5   0.10         100   3.141602e+04   7.271427e-01   5.883993e+04   \n",
      "6   0.50          10   1.464434e+17  -1.271904e+12   1.632452e+17   \n",
      "7   0.50          50   1.293867e+67  -1.123761e+62   1.442316e+67   \n",
      "8   0.50         100  3.504812e+129 -3.044031e+124  3.906928e+129   \n",
      "\n",
      "        Test R^2  \n",
      "0  -6.965872e-01  \n",
      "1   4.342845e-01  \n",
      "2   6.324587e-01  \n",
      "3   6.401074e-01  \n",
      "4   6.467845e-01  \n",
      "5   6.470881e-01  \n",
      "6  -9.791172e+11  \n",
      "7  -8.650767e+61  \n",
      "8 -2.343309e+124  \n"
     ]
    }
   ],
   "source": [
    "# add intercepts\n",
    "X_train_scaled = np.hstack([np.ones((X_train_scaled.shape[0], 1)), X_train_scaled])\n",
    "X_test_scaled = np.hstack([np.ones((X_test_scaled.shape[0], 1)), X_test_scaled])\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "iterations_list = [10, 50, 100]\n",
    "\n",
    "for alpha in learning_rates: # for each learning rate, go through diff num of iterations\n",
    "    for num_iters in iterations_list:\n",
    "        theta = gradient_descent(X_train_scaled, y_train, alpha, num_iters)\n",
    "\n",
    "        y_train_pred = X_train_scaled @ theta\n",
    "        y_test_pred = X_test_scaled @ theta\n",
    "\n",
    "        results.append({\n",
    "            \"alpha\": alpha,\n",
    "            \"iterations\": num_iters,\n",
    "            \"Train MSE\": mean_squared_error(y_train, y_train_pred),\n",
    "            \"Train R^2\": r2_score(y_train, y_train_pred),\n",
    "            \"Test MSE\": mean_squared_error(y_test, y_test_pred),\n",
    "            \"Test R^2\": r2_score(y_test, y_test_pred)\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61126525",
   "metadata": {},
   "source": [
    "3. Write some observations about the behavior of the algorithm: How do the metrics change with different learning rates; How many\n",
    "iterations are needed; Does the algorithm converge to the optimal solution, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71331b67",
   "metadata": {},
   "source": [
    "The results show that the learning rate significantly affects the convergence behavior of gradient descent. \n",
    "\n",
    "When α = 0.01, the algorithm converges slowly: performance is poor at 10 iterations but steadily improves by 100 iterations, indicating gradual movement toward the optimal solution.\n",
    "\n",
    "When α = 0.1, the algorithm converges much faster, achieving strong performance within 10–50 iterations, with little improvement afterward, suggesting it has reached the minimum. \n",
    "\n",
    "However, when α = 0.5, the algorithm diverges, as shown by extremely large MSE values and highly negative (R^2), meaning the learning rate is too large and causes overshooting. \n",
    "\n",
    "Overall, α = 0.1 provides the best balance and demonstrates that an appropriate learning rate allows gradient descent to efficiently converge to the optimal solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
