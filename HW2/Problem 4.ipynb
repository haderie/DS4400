{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6ef580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"/Users/haderie/Downloads/housing/train.csv\")\n",
    "train_df = train_df.drop(columns=[ \"zipcode\"])\n",
    "\n",
    "X_train = train_df.drop(columns=[\"price\"]) # features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "y_train = train_df[\"price\"] / 1000 # target\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(\"/Users/haderie/Downloads/housing/test.csv\")\n",
    "test_df = test_df.drop(columns=[\"id\", \"date\", \"zipcode\"])\n",
    "\n",
    "X_test = test_df.drop(columns=[\"price\"]) # featurs\n",
    "\n",
    "y_test = test_df[\"price\"] / 1000  # target\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74cf42",
   "metadata": {},
   "source": [
    "- Consider a feature $X$, a response variable $Y$, and $N$ samples of training data. Implement a polynomial regression model that fits a polynomial of degree $p$ to the data using the least-square method. Use your own implementation from Problem 3 and adapt it for polynomial\n",
    "regression. If $p=2$, the model will use two features ($X$ and $X^2$), if $p=3$ the model will use 3 features ($X,X^2,X^3$), and so on for larger values of $p$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03d01e37-0b45-4bdf-a046-5f8b1b398182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_design_matrix(x, p):\n",
    "    \"\"\"\n",
    "    p - polynomial degree\n",
    "    returns: (N, p+1) design matrix [1, x, x^2, ..., x^p]\n",
    "    \"\"\"\n",
    "    return np.column_stack([x**i for i in range(p + 1)])\n",
    "\n",
    "def poly_reg_closed_form(x, y, p):\n",
    "    \"\"\"\n",
    "    Closed-form polynomial regression\n",
    "    \"\"\"\n",
    "    X = poly_design_matrix(x, p)\n",
    "    theta = np.linalg.pinv(X) @ y\n",
    "    return theta\n",
    "\n",
    "def predict_poly(x, theta, p):\n",
    "    X = poly_design_matrix(x, p)\n",
    "    return X @ theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b5ca9e",
   "metadata": {},
   "source": [
    "- Consider the house price prediction problem with feature $X=$ `sqft_living`. Train a polynomial regression model for different values of $p \\le 5$ using your implementation. Include a table with the MSE and $R^2$ metrics on both the training and testing data for at least 3 different values of $p$. Discuss your observations on how the MSE and $R^2$ metrics change with the degree of the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d36ac319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Degree p     Train MSE  Train R^2       Test MSE  Test R^2\n",
      "0         1  57947.526161   0.496709   88575.978543  0.468736\n",
      "1         2  54822.665116   0.523849   71791.679479  0.569406\n",
      "2         3  53785.194716   0.532860   99833.483763  0.401216\n",
      "3         5  52626.111955   0.542927  570616.914821 -2.422464\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "X_train = train_df[[\"sqft_living\"]]\n",
    "y_train = train_df[\"price\"] / 1000\n",
    "\n",
    "X_test = test_df[[\"sqft_living\"]]\n",
    "y_test = test_df[\"price\"] / 1000\n",
    "\n",
    "# standardize feature\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "for p in [1, 2, 3, 5]:\n",
    "    \n",
    "    theta = poly_reg_closed_form(X_train, y_train, p)\n",
    "\n",
    "    y_train_pred = predict_poly(X_train, theta, p)\n",
    "    y_test_pred  = predict_poly(X_test_scaled, theta, p)\n",
    "\n",
    "\n",
    "    # metric\n",
    "    results.append({\n",
    "        \"Degree p\": p,\n",
    "        \"Train MSE\": mean_squared_error(y_train, y_train_pred),\n",
    "        \"Train R^2\": r2_score(y_train, y_train_pred),\n",
    "        \"Test MSE\": mean_squared_error(y_test, y_test_pred),\n",
    "        \"Test R^2\": r2_score(y_test, y_test_pred)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45faa7",
   "metadata": {},
   "source": [
    "As the polynomial degree increases, the training MSE decreases and the training R² increases, indicating that the model becomes more flexible and fits the training data better. \n",
    "\n",
    "However, the testing performance improves only up to degree 2, after which it deteriorates significantly. For degree 5, the testing MSE becomes extremely large and the R² becomes negative, indicating severe overfitting. \n",
    "\n",
    "Increasing model complexity reduces bias but increases variance, which harms generalization performance. P=2 provides the best balance between bias and variance for this feature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
